{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Base API URL for Apache Jira\n",
        "JIRA_API_BASE = 'https://issues.apache.org/jira/rest/api/2'\n",
        "\n",
        "# Projects to scrape\n",
        "PROJECTS = ['HADOOP', 'SPARK', 'KAFKA']\n",
        "\n",
        "# Max issues per request\n",
        "PAGE_SIZE = 50\n",
        "\n",
        "# Number of retries for failed requests\n",
        "MAX_RETRIES = 5\n",
        "\n",
        "# File to save scraper checkpoint state\n",
        "CHECKPOINT_FILE = 'jira_scraper_checkpoint.json'\n",
        "\n",
        "# Headers needed to accept JSON responses\n",
        "HEADERS = {'Accept': 'application/json'}\n",
        "\n",
        "class JiraDataScraper:\n",
        "    def __init__(self, projects):\n",
        "        self.projects = projects\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update(HEADERS)\n",
        "        self.checkpoint = self.load_checkpoint()\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        \"\"\"Load last scraping positions from a checkpoint file to resume.\"\"\"\n",
        "        if os.path.exists(CHECKPOINT_FILE):\n",
        "            with open(CHECKPOINT_FILE, 'r') as f:\n",
        "                return json.load(f)\n",
        "        else:\n",
        "            # Initialize with zero offsets for all projects\n",
        "            return {project: 0 for project in self.projects}\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        \"\"\"Save current state to a checkpoint file.\"\"\"\n",
        "        with open(CHECKPOINT_FILE, 'w') as f:\n",
        "            json.dump(self.checkpoint, f)\n",
        "\n",
        "    def get_json_with_retries(self, url, params=None):\n",
        "        \"\"\"Generic HTTP GET with retries, handling rate limit and server errors.\"\"\"\n",
        "        for attempt in range(MAX_RETRIES):\n",
        "            try:\n",
        "                response = self.session.get(url, params=params, timeout=15)\n",
        "                if response.status_code == 200:\n",
        "                    return response.json()\n",
        "                elif response.status_code == 429:\n",
        "                    wait_time = int(response.headers.get('Retry-After', '10'))\n",
        "                    print(f\"Rate limit hit. Sleeping for {wait_time} seconds.\")\n",
        "                    time.sleep(wait_time)\n",
        "                elif 500 <= response.status_code < 600:\n",
        "                    backoff = 2 ** attempt\n",
        "                    print(f\"Server error {response.status_code}. Retrying in {backoff} seconds.\")\n",
        "                    time.sleep(backoff)\n",
        "                else:\n",
        "                    print(f\"Unexpected status code: {response.status_code}. Aborting request.\")\n",
        "                    break\n",
        "            except requests.RequestException as e:\n",
        "                backoff = 2 ** attempt\n",
        "                print(f\"Request failed with {e}. Retrying after {backoff} seconds.\")\n",
        "                time.sleep(backoff)\n",
        "        return None\n",
        "\n",
        "    def fetch_issues(self, project, start_at):\n",
        "        \"\"\"Fetch a page of issues for project starting at start_at offset.\"\"\"\n",
        "        url = f\"{JIRA_API_BASE}/search\"\n",
        "        jql_query = f\"project={project} order by created asc\"\n",
        "        params = {\n",
        "            'jql': jql_query,\n",
        "            'startAt': start_at,\n",
        "            'maxResults': PAGE_SIZE,\n",
        "            'fields': 'summary,status,assignee,reporter,priority,labels,created,updated,description'\n",
        "        }\n",
        "        return self.get_json_with_retries(url, params)\n",
        "\n",
        "    def fetch_comments(self, issue_key):\n",
        "        \"\"\"Fetch comments associated with an issue.\"\"\"\n",
        "        url = f\"{JIRA_API_BASE}/issue/{issue_key}/comment\"\n",
        "        data = self.get_json_with_retries(url)\n",
        "        if data and 'comments' in data:\n",
        "            return data['comments']\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    def scrape_project_issues(self, project):\n",
        "        \"\"\"Scrape all issues and comments for a single project.\"\"\"\n",
        "        start_at = self.checkpoint.get(project, 0)\n",
        "        collected_issues = []\n",
        "\n",
        "        while True:\n",
        "            data = self.fetch_issues(project, start_at)\n",
        "            if not data:\n",
        "                print(f\"Failed to fetch issues for project {project} at offset {start_at}. Stopping.\")\n",
        "                break\n",
        "\n",
        "            issues = data.get('issues', [])\n",
        "            if not issues:\n",
        "                print(f\"No more issues found for project {project}. Completed.\")\n",
        "                break\n",
        "\n",
        "            for issue in issues:\n",
        "                issue_key = issue.get('key')\n",
        "                fields = issue.get('fields', {})\n",
        "\n",
        "                comments_raw = self.fetch_comments(issue_key)\n",
        "                comments = [\n",
        "                    {\n",
        "                        'author': c.get('author', {}).get('displayName') if c.get('author') else None,\n",
        "                        'created': c.get('created'),\n",
        "                        'body': c.get('body', '')\n",
        "                    }\n",
        "                    for c in comments_raw\n",
        "                ]\n",
        "\n",
        "                issue_data = {\n",
        "                    'issue_id': issue_key,\n",
        "                    'project': project,\n",
        "                    'title': fields.get('summary'),\n",
        "                    'status': fields.get('status', {}).get('name'),\n",
        "                    'reporter': (fields.get('reporter') or {}).get('displayName'),\n",
        "                    'assignee': (fields.get('assignee') or {}).get('displayName'),\n",
        "                    'priority': (fields.get('priority') or {}).get('name'),\n",
        "                    'labels': fields.get('labels', []),\n",
        "                    'created': fields.get('created'),\n",
        "                    'updated': fields.get('updated'),\n",
        "                    'description': fields.get('description', '') or '',\n",
        "                    'comments': comments,\n",
        "                    # Placeholder for LLM tasks (to fill in later)\n",
        "                    'derived_tasks': {\n",
        "                        'summarization': '',\n",
        "                        'classification': '',\n",
        "                        'qa': []\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                collected_issues.append(issue_data)\n",
        "\n",
        "            start_at += PAGE_SIZE\n",
        "            self.checkpoint[project] = start_at\n",
        "            self.save_checkpoint()\n",
        "\n",
        "            if start_at >= data.get('total', 0):\n",
        "                print(f\"Finished all pages for project {project}.\")\n",
        "                break\n",
        "\n",
        "        return collected_issues\n",
        "\n",
        "def main():\n",
        "    scraper = JiraDataScraper(PROJECTS)\n",
        "    for proj in PROJECTS:\n",
        "        print(f\"Scraping project: {proj}\")\n",
        "        issues = scraper.scrape_project_issues(proj)\n",
        "\n",
        "        # Append issues as JSONL to file\n",
        "        filename = f\"{proj}_issues.jsonl\"\n",
        "        with open(filename, 'a', encoding='utf-8') as f:\n",
        "            for issue in issues:\n",
        "                f.write(json.dumps(issue) + '\\n')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gkssjobhIrs",
        "outputId": "03c9ec28-7cce-40b9-a18c-8ca512611099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping project: HADOOP\n"
          ]
        }
      ]
    }
  ]
}